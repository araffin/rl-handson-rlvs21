{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rlvs_hands_on_sb3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Stable Baselines3 Hands-on Session - RLVS\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-handson-rlvs21\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn the basics for using stable baselines3 library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
        "You will also learn how to define a gym wrapper and callback to customise the training.\n",
        "We will finish this session by trying out multiprocessing and have a hyperparameter tuning challenge.\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYdv2ygjLaFL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oexj67yWN5_k"
      },
      "source": [
        "# Optional: install SB3 contrib to have access to additional algorithms\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-khNkrgcI6Z1"
      },
      "source": [
        "# Part I: Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzevZcgmJmhi"
      },
      "source": [
        "## First steps with the Gym interface\n",
        "\n",
        "An environment that follows the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html) is quite simple to use.\n",
        "It provides to this user mainly three methods:\n",
        "- `reset()` called at the beginning of an episode, it returns an observation\n",
        "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether the episode is over and additional information\n",
        "- (Optional) `render(method='human')` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `method='rbg_array'` to retrieve an image of the scene\n",
        "\n",
        "Under the hood, it also contains two useful properties:\n",
        "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
        "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
        "\n",
        "The best way to learn about gym spaces is to look at the [source code](https://github.com/openai/gym/tree/master/gym/spaces), but you need to know at least the main ones:\n",
        "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
        "```python\n",
        "# Example for using image as input:\n",
        "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
        "```                                       \n",
        "\n",
        "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
        "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1.\n",
        "\n",
        "\n",
        "\n",
        "[Documentation on custom env](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)\n",
        "\n",
        "Below you can find an example of a custom environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owrHOGhlMGLE"
      },
      "source": [
        "from typing import Any, Callable, Dict, List, NamedTuple, Tuple, Union\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "GymObs = Union[Tuple, Dict, np.ndarray, int]\n",
        "\n",
        "class CustomEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Minimal custom environment to demonstrate the Gym interface.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(CustomEnv, self).__init__()\n",
        "    self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(14,))\n",
        "    self.action_space = gym.spaces.Box(low=-1, high=1, shape=(6,))\n",
        "\n",
        "  def reset(self) -> GymObs:\n",
        "    \"\"\"\n",
        "    Called at the beginning of an episode.\n",
        "    :return: the first observation of the episode\n",
        "    \"\"\"\n",
        "    return self.observation_space.sample()\n",
        "\n",
        "  def step(self, action: Union[int, np.ndarray]) -> Tuple[GymObs, float, bool, Dict]:\n",
        "    \"\"\"\n",
        "    Step into the environment.\n",
        "    :return: A tuple containing the new observation, the reward signal, \n",
        "      whether the episode is over and additional informations.\n",
        "    \"\"\"\n",
        "    obs = self.observation_space.sample()\n",
        "    reward = 1.0\n",
        "    done = False\n",
        "    info = {}\n",
        "    return obs, reward, done, info\n",
        "\n",
        "env = CustomEnv()\n",
        "# Check your custom environment\n",
        "# this will print warnings and throw errors if needed\n",
        "check_env(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcX8hEcaUpR0"
      },
      "source": [
        "Stable-Baselines3 works on environments that follow the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
        "You can find a list of available environment [here](https://gym.openai.com/envs/#classic_control).\n",
        "\n",
        "It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym does not have a proper documentation.\n",
        "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7tKaBFrTR0a"
      },
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcsXmYRMON9W"
      },
      "source": [
        "# Algorithms from the contrib repo\n",
        "# https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "from sb3_contrib import QRDQN, TQC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
        "This step is optional as you can directly use strings in the constructor: \n",
        "\n",
        "```PPO(\"MlpPolicy\", env)``` instead of ```PPO(MlpPolicy, env)```\n",
        "\n",
        "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommended option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROUJr675TT01"
      },
      "source": [
        "from stable_baselines3.ppo.policies import MlpPolicy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Create the Gym env and instantiate the agent\n",
        "\n",
        "For this example, we will use CartPole environment, a classic control problem.\n",
        "\n",
        "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
        "\n",
        "Cartpole environment: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n",
        "\n",
        "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
        "\n",
        "\n",
        "We chose the MlpPolicy because the observation of the CartPole task is a feature vector, not images.\n",
        "\n",
        "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
        "\n",
        "Here we are using the [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
        "\n",
        "It combines ideas from [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
        "\n",
        "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
        "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "source": [
        "# Create the gym Env\n",
        "\n",
        "# Create the RL agent\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0K2YzpGDwQ4"
      },
      "source": [
        "### Using the model to predict actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRfb_f7uD-H9"
      },
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF_xsDEADvv_"
      },
      "source": [
        "# Retrieve first observation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgrRoM23EU9Z"
      },
      "source": [
        "# Predict the action to take given the observation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1wRSvPrEjUa"
      },
      "source": [
        "# We are using discrete actions, therefore `action` is an int\n",
        "assert env.action_space.contains(action)\n",
        "\n",
        "print(action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtEtKqKEvFC"
      },
      "source": [
        "Step in the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmJ-yv5MEllY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTNOOlOdE1lH"
      },
      "source": [
        "print(f\"obs_shape={obs.shape}, reward={reward}, done? {done}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dd6hT873ImE"
      },
      "source": [
        "# Reset the env at the end of an episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSeXppUFNLg"
      },
      "source": [
        "### Exercise (10 minutes): write the function to evaluate the agent\n",
        "\n",
        "This function will be used to evaluate the performance of an RL agent.\n",
        "Thanks to Stable Baselines3 interface, it will work with any SB3 algorithms and any Gym environment.\n",
        "\n",
        "See docstring of the function for what is expected as input/output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf7Ktha2FE3B"
      },
      "source": [
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: BaseAlgorithm,\n",
        "    env: gym.Env,\n",
        "    n_eval_episodes: int = 100,\n",
        "    deterministic: bool = False,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Evaluate an RL agent for `n_eval_episodes`.\n",
        "\n",
        "    :param model: the RL Agent\n",
        "    :param env: the gym Environment\n",
        "    :param n_eval_episodes: number of episodes to evaluate it\n",
        "    :param deterministic: Whether to use deterministic or stochastic actions\n",
        "    :return: Mean reward for the last `n_eval_episodes`\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    # TODO: run `n_eval_episodes` episodes in the Gym env\n",
        "    # using the RL agent and keep track of the total reward\n",
        "    # collected for each episode.\n",
        "    # Finally, compute the mean and print it\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "    return mean_episode_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3SahgtZH2TX"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XGmJK4oHUeT",
        "outputId": "f3b1fbf7-d6c3-470c-e6ae-8e61a3fe4be6"
      },
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, env, n_eval_episodes=100, deterministic=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 21.45 Num episodes: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjjPxrwkYJ2i"
      },
      "source": [
        "Stable-Baselines already provides you with that helper (the actual implementation is a little more advanced):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z6K9YImYJEx"
      },
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lHSorUyH8I8"
      },
      "source": [
        "# The Monitor wrapper allows to keep track of the training reward and other infos (useful for plotting)\n",
        "env = Monitor(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oPTHjxyZSOL"
      },
      "source": [
        "# Seed to compare to previous implementation\n",
        "env.seed(42)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Train the agent and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "source": [
        "# Train the agent for 10000 steps\n",
        "model.learn(total_timesteps=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "Apparently the training went well, the mean reward increased a lot! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Prepare video recording"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you can learn more about those wrappers in our Documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### Visualize trained agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iATu7AiyMQW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb0c463-d31c-49e0-9ea9-5cf9ae94c5d4"
      },
      "source": [
        "record_video('CartPole-v1', model, video_length=500, prefix='ppo-cartpole')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving video to  /content/videos/ppo-cartpole-step-0-to-step-500.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n4i-fW3NojZ"
      },
      "source": [
        "show_videos('videos', prefix='ppo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5WHto5hNaZN"
      },
      "source": [
        "### Exercise (5 minutes): Save, Load The Model and that the loading was correct\n",
        "\n",
        "Save the model and then load it.\n",
        "\n",
        "Don't forget to check that loading went well: the model must predict the same actions given the same  observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCcS6wJ6Nurd"
      },
      "source": [
        "# Sample observations using the environment observation space\n",
        "\n",
        "# Predict actions on those observations using trained model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWCUZSD7NZ_Y"
      },
      "source": [
        "# Save the model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2M1S_-V2gMi"
      },
      "source": [
        "# Delete the model (to demonstrate loading)\n",
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ne1MlIiNpee",
        "outputId": "0350fd76-db7b-4943-c7e1-9bfa9a4c7ddd"
      },
      "source": [
        "!ls *.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ppo_cartpole.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55G-6nJLNrPk"
      },
      "source": [
        "# Load the model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIWII5YxNtrI"
      },
      "source": [
        "# Predict actions on the observations with the loaded model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m06ipphZN_0E"
      },
      "source": [
        "# Check that the predictions are the same\n",
        "assert np.allclose(action_before_saving, action_after_loading), \"Somethng went wrong in the loading\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8zg4V566qD"
      },
      "source": [
        "## Bonus: Train a RL Model in One Line\n",
        "\n",
        "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaOPfOrwWEP4"
      },
      "source": [
        "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm4RncfCJFyP"
      },
      "source": [
        "# Part II: Gym Wrappers\n",
        "\n",
        "\n",
        "In this part, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds4AAfmISQIA"
      },
      "source": [
        "## Anatomy of a gym wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTS9e9hTzZZ"
      },
      "source": [
        "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
        "\n",
        "Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n",
        "There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYo0C0TQSL3c"
      },
      "source": [
        "class CustomWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env:  Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env: gym.Env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super().__init__(env)\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    obs = self.env.reset()\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    obs, reward, done, infos = self.env.step(action)\n",
        "    return obs, reward, done, infos\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7RkTLshPMGq"
      },
      "source": [
        "### Exercise (7 minutes): limit the episode length\n",
        "\n",
        "In this exercise, the goal is to create a Gym wrapper that will limit the maximum number of steps per episode (timeout).\n",
        "\n",
        "\n",
        "It will also pass a `timeout` signal in the info dict to tell the agent that the termination was due to reaching the limits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h_ypIxgPLGL"
      },
      "source": [
        "class TimeLimitWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  Limit the maximum number of steps per episode.\n",
        "\n",
        "  :param env: Gym environment that will be wrapped\n",
        "  :param max_steps: Max number of steps per episode\n",
        "  \"\"\"\n",
        "  def __init__(self, env: gym.Env, max_steps: int = 100):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super().__init__(env)\n",
        "    self.max_steps = max_steps\n",
        "    # YOUR CODE HERE\n",
        "    # Counter of steps per episode\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "  \n",
        "  def reset(self) -> GymObs:\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: reset the counter and reset the env\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "    return obs\n",
        "\n",
        "  def step(self, action: Union[int, np.ndarray]) -> Tuple[GymObs, float, bool, Dict]:\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: \n",
        "    # 1. Step into the env\n",
        "    # 2. Increment the episode counter\n",
        "    # 3. Overwrite the done signal when time limit is reached \n",
        "    # (optional) 4. update the info dict (add a \"episode_timeout\" key)\n",
        "    # when the episode was stopped due to timelimit\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "    return obs, reward, done, infos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FQptLSuPB3A"
      },
      "source": [
        "#### Test the wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szZ43D5PVB07"
      },
      "source": [
        "from gym.envs.classic_control.pendulum import PendulumEnv\n",
        "\n",
        "# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n",
        "env = PendulumEnv()\n",
        "# Wrap the environment\n",
        "env = TimeLimitWrapper(env, max_steps=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cencka9iVg9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ddfdb8-be4c-4df9-e052-d4a54666419e"
      },
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "n_steps = 0\n",
        "while not done:\n",
        "  # Take random actions\n",
        "  random_action = env.action_space.sample()\n",
        "  obs, reward, done, infos = env.step(random_action)\n",
        "  n_steps += 1\n",
        "\n",
        "print(f\"Episode length: {n_steps} steps, info dict: {infos}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode length: 100 steps, info dict: {'episode_timeout': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkMYA63sV9aA"
      },
      "source": [
        "In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yms4z_GaQ6dz"
      },
      "source": [
        "# Part III: Callbacks\n",
        "\n",
        "In this part, you will learn how to use [Callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html) which allow to do monitoring, auto saving, model manipulation, progress bars, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHk8FXdRUnw"
      },
      "source": [
        "Please read the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html). Although Stable-Baselines3 provides you with a callback collection (e.g. for creating checkpoints or for evaluation), we are going to re-implement some so you can get a good understanding of how they work.\n",
        "\n",
        "To build a custom callback, you need to create a class that derives from `BaseCallback`. This will give you access to events (`_on_training_start`, `_on_step()`) and useful variables (like `self.model` for the RL model).\n",
        "\n",
        "`_on_step` returns a boolean value for whether or not the training should continue.\n",
        "\n",
        "Thanks to the access to the models variables, in particular `self.model`, we are able to even change the parameters of the model without halting the training, or changing the model's code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE30k2i7kohh"
      },
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjRvJ8zBftL3"
      },
      "source": [
        "class CustomCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    A custom callback that derives from ``BaseCallback``.\n",
        "\n",
        "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose=0):\n",
        "        super(CustomCallback, self).__init__(verbose)\n",
        "        # Those variables will be accessible in the callback\n",
        "        # (they are defined in the base class)\n",
        "        # The RL model\n",
        "        # self.model = None  # type: BaseRLModel\n",
        "        # An alias for self.model.get_env(), the environment used for training\n",
        "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
        "        # Number of time the callback was called\n",
        "        # self.n_calls = 0  # type: int\n",
        "        # self.num_timesteps = 0  # type: int\n",
        "        # local and global variables\n",
        "        # self.locals = None  # type: Dict[str, Any]\n",
        "        # self.globals = None  # type: Dict[str, Any]\n",
        "        # The logger object, used to report things in the terminal\n",
        "        # self.logger = None  # type: logger.Logger\n",
        "        # # Sometimes, for event callback, it is useful\n",
        "        # # to have access to the parent object\n",
        "        # self.parent = None  # type: Optional[BaseCallback]\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_rollout_start(self) -> None:\n",
        "        \"\"\"\n",
        "        A rollout is the collection of environment interaction\n",
        "        using the current policy.\n",
        "        This event is triggered before collecting new samples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        \"\"\"\n",
        "        This method will be called by the model after each call to `env.step()`.\n",
        "\n",
        "        For child callback (of an `EventCallback`), this will be called\n",
        "        when the event is triggered.\n",
        "\n",
        "        :return: If the callback returns False, training is aborted early.\n",
        "        \"\"\"\n",
        "        return True\n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before updating the policy.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqpPtxaCfynB"
      },
      "source": [
        "Here we have a simple callback that can only be called twice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILY0AkFfzPJ"
      },
      "source": [
        "class SimpleCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    a simple callback that can only be called twice\n",
        "\n",
        "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose=0):\n",
        "        super(SimpleCallback, self).__init__(verbose)\n",
        "        self._called = False\n",
        "    \n",
        "    def _on_step(self):\n",
        "      \n",
        "      if not self._called:\n",
        "        print(\"callback - first call\")\n",
        "        self._called = True\n",
        "        return True # returns True, training continues.\n",
        "\n",
        "      print(\"callback - second call\")\n",
        "      return False # returns False, training stops.      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gTXaNLARUnw"
      },
      "source": [
        "model = SAC('MlpPolicy', 'Pendulum-v1', verbose=1)\n",
        "model.learn(8000, callback=SimpleCallback())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adsKMvDkRUn0"
      },
      "source": [
        "## Exercise (8 minutes): Checkpoint Callback\n",
        "\n",
        "In RL, it is quite useful to save checkpoints during training, as we can end up with burn-in of a bad policy. It also useful if you want to see the progression over time.\n",
        "\n",
        "This is a typical use case for callback, as they can call the save function of the model, and observe the training over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDI3lKTiiKP9"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzMHj7r3h78m"
      },
      "source": [
        "class CheckpointCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model every ``save_freq`` steps\n",
        "\n",
        "    :param save_freq:\n",
        "    :param save_path: Path to the folder where the model will be saved.\n",
        "    :param name_prefix: Common prefix to the saved models\n",
        "    :param verbose: Whether to print additional infos or not\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, save_freq: int, save_path: str, name_prefix: str = \"rl_model\", verbose: int = 0):\n",
        "        super().__init__(verbose)\n",
        "        self.save_freq = save_freq\n",
        "        self.save_path = save_path\n",
        "        self.name_prefix = name_prefix\n",
        "        # NOTE: because it derives from `BaseCallback`\n",
        "        # this checkpoint callback has already access to many variables\n",
        "        # like `self.model` (cf ``CustomCallback above for a complete list)\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        ## YOUR CODE HERE\n",
        "        # Create folder if needed\n",
        "        # (you may use `os.makedirs()`)\n",
        "\n",
        "        ## END OF YOUR CODE\n",
        "\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        ## YOUR CODE HERE\n",
        "        # Save the checkpoint if needed\n",
        "\n",
        "        ## END OF YOUR CODE\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3krf5FCaLYn"
      },
      "source": [
        "Test your callback:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TuYLBEaRUn0"
      },
      "source": [
        "log_dir = \"/tmp/gym/\"\n",
        "# Create Callback\n",
        "callback = CheckpointCallback(save_freq=1000, save_path=\"/tmp/gym/\", verbose=1)\n",
        "\n",
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=1)\n",
        "model.learn(total_timesteps=5000, callback=callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc_z8jF-TFQP"
      },
      "source": [
        "!ls \"/tmp/gym/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0_iiocKUTW0"
      },
      "source": [
        "Note: The `CheckpointCallback` as well as other [common callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html), like the `EvalCallback` are already included in Stable-Baselines3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4TavoikVlCt"
      },
      "source": [
        "## Multiprocessing Demo\n",
        "\n",
        "\n",
        "[Vectorized Environments](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html) are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. This provides two benefits:\n",
        "* Agent experience can be collected more quickly\n",
        "* The experience will contain a more diverse range of states, it usually improves exploration\n",
        "\n",
        "Stable-Baselines provides two types of Vectorized Environment:\n",
        "- SubprocVecEnv which run each environment in a separate process\n",
        "- DummyVecEnv which run all environment on the same process\n",
        "\n",
        "In practice, DummyVecEnv is usually faster than SubprocVecEnv because of communication delays that subprocesses have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7obQMBgXV8_z"
      },
      "source": [
        "import time\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1K_YuEfVnT6"
      },
      "source": [
        "env = gym.make(\"Pendulum-v1\")\n",
        "n_steps = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBa5rJfiWFX3"
      },
      "source": [
        "start_time_one_env = time.time()\n",
        "model = PPO(\"MlpPolicy\", env, n_epochs=1, n_steps=n_steps, verbose=1).learn(int(2e4))\n",
        "time_one_env = time.time() - start_time_one_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylbC5slbWzKo",
        "outputId": "2f7cc82e-224d-4d5a-8803-a31a9013cf71"
      },
      "source": [
        "print(f\"Took {time_one_env:.2f}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 20.17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0UgmDOHWBWU"
      },
      "source": [
        "start_time_vec_env = time.time()\n",
        "# Create 16 environments\n",
        "vec_env = make_vec_env(\"Pendulum-v1\", n_envs=16)\n",
        "# At each call to `env.step()`, 16 transitions will be collected, so we account for that for fair comparison\n",
        "model = PPO(\"MlpPolicy\", vec_env, n_epochs=1, n_steps=n_steps // 16, verbose=1).learn(int(2e4))\n",
        "\n",
        "time_vec_env = time.time() - start_time_vec_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN3o5TJ3WxOy",
        "outputId": "0018d2aa-a8b0-474a-a23f-b4151debba42"
      },
      "source": [
        "print(f\"Took {time_vec_env:.2f}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 5.01s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUrDxSkJT_gn"
      },
      "source": [
        "Note: the speedup is not linear but it is already significant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwFOp0j-ga-_"
      },
      "source": [
        "# Part IV: The importance of hyperparameter tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PytOtL9GdmrE"
      },
      "source": [
        "When compared with Supervised Learning, Deep Reinforcement Learning is far more sensitive to the choice of hyper-parameters such as learning rate, number of neurons, number of layers, optimizer ... etc. \n",
        "\n",
        "Poor choice of hyper-parameters can lead to poor/unstable convergence. This challenge is compounded by the variability in performance across random seeds (used to initialize the network weights and the environment).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8PNN9kcgolk"
      },
      "source": [
        "### Challenge (15 minutes): \"Grad Student Descent\" - Can you beat automatic hyperparameter tuning?\n",
        "\n",
        "The challenge is to find the best hyperparameters (max performance) for A2C on `CartPole-v1` with a limited budget of 20 000 training steps.\n",
        "\n",
        "You will compete against automatic hyperparameter tuning, good luck ;)\n",
        "\n",
        "\n",
        "Maximum reward: 500 on `CartPole-v1`\n",
        "\n",
        "The hyperparameters should work for different random seeds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aqxsini7H3"
      },
      "source": [
        "budget = int(2e4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDQ805DBi3KM"
      },
      "source": [
        "#### The baseline: default hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1PSNGcsi2dP"
      },
      "source": [
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1).learn(budget)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d3X0G0ng2OE"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=50, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-fi1-oKnUI2"
      },
      "source": [
        "**Your goal is to beat that baseline and get closer to the optimal score of 500**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvq8zizok1X_"
      },
      "source": [
        "Time to tune!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaqCCH4gkRH_"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDUfeZcyjPKS"
      },
      "source": [
        "policy_kwargs = dict(\n",
        "    net_arch=[\n",
        "      dict(vf=[64, 64], pi=[64, 64]), # network architectures for actor/critic\n",
        "    ],\n",
        "    ortho_init=True, # Orthogonal initialization,\n",
        "    activation_fn=nn.Tanh,\n",
        ")\n",
        "\n",
        "hyperparams = dict(\n",
        "    n_steps=5,\n",
        "    learning_rate=7e-4,\n",
        "    gamma=0.99, # discount factor\n",
        "    gae_lambda=1.0, # Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
        "                    # Equivalent to classic advantage when set to 1.\n",
        "    max_grad_norm=0.5, # The maximum value for the gradient clipping\n",
        "    ent_coef=0.0, # Entropy coefficient for the loss calculation\n",
        ")\n",
        "\n",
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1, **hyperparams).learn(budget)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YHvzTJArTGU"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=50, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL_G9DurUV75"
      },
      "source": [
        "Hint - Recommended Hyperparameter Range\n",
        "\n",
        "```python\n",
        "gamma = trial.suggest_float(\"gamma\", 0.9, 0.99999, log=True)\n",
        "max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
        "gae_lambda = trial.suggest_float(\"gae_lambda\", 0.8, 0.999, log=True)\n",
        "# from 2**3 = 8 to 2**10 = 1024\n",
        "n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
        "learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
        "ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
        "ortho_init = trial.suggest_categorical(\"ortho_init\", [False, True])\n",
        "# tiny: {\"pi\": [64], \"vf\": [64]}\n",
        "# default: {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
        "net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"default\"])\n",
        "activation_fn = trial.suggest_categorical(\"activation_fn\", [nn.Tanh, nn.ReLU])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCbep6z1h3D1"
      },
      "source": [
        "Simple example of hyperparameter tuning: https://github.com/optuna/optuna/blob/master/examples/rl/sb3_simple.py\n",
        "\n",
        "Complete example: https://github.com/DLR-RM/rl-baselines3-zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yUeYnfJVpB2"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "What we have seen in this notebook:\n",
        "- SB3 101\n",
        "- Gym wrappers to modify the env\n",
        "- SB3 callbacks to access the RL agent\n",
        "- multiprocessing to speedup training\n",
        "- the importance of good hyperparameters\n",
        "- more complete tutorial: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-gqIPXqV7zZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
